<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><title>GDG ML Papers</title><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Libre+Bodoni:ital,wght@0,400..700;1,400..700&family=UnifrakturMaguntia&display=swap" rel="stylesheet"><meta name="astro-view-transitions-enabled" content="true"><meta name="astro-view-transitions-fallback" content="animate"><script type="module" src="/gdg-ml-papers/_astro/ClientRouter.astro_astro_type_script_index_0_lang.CMTcOisY.js"></script><style>[data-astro-cid-kk57j3mo]{box-sizing:border-box}html{scroll-behavior:smooth;font-size:62.5%}body{background-color:#d9d9d9;color:#373737;margin:0;padding:3%}.btn[data-astro-cid-kk57j3mo]{margin-top:32px;font-size:1.6rem;background-color:#373737;color:#d9d9d9;text-decoration:none;padding:12px 24px;border-radius:8px;cursor:pointer;transition:all .4s ease-in-out}.btn[data-astro-cid-kk57j3mo]:hover{background-color:#d9d9d9;color:#373737;border:2px solid black;box-shadow:0 6px 24px #0003;transition:all .6s ease-in-out}h1[data-astro-cid-kk57j3mo]{font-size:3rem;line-height:1;margin:12px 0}h2[data-astro-cid-kk57j3mo]{margin:50px 0 0;padding:0}h3[data-astro-cid-kk57j3mo]{font-family:UnifrakturMaguntia,cursive;color:#373737;font-size:1.8rem}a[data-astro-cid-kk57j3mo]{text-decoration:none}.content[data-astro-cid-kk57j3mo]{border:2px solid #6a6a6a;border-radius:4px;padding:1% 4%;font-size:1.8rem}code[data-astro-cid-kk57j3mo]{font-size:1.4rem;color:#6a6a6a;display:block}.resume[data-astro-cid-kk57j3mo]{margin-top:32px;padding:0 4px 4px}
.astro-route-announcer{position:absolute;left:0;top:0;clip:rect(0 0 0 0);clip-path:inset(50%);overflow:hidden;white-space:nowrap;width:1px;height:1px}@keyframes astroFadeInOut{0%{opacity:1}to{opacity:0}}@keyframes astroFadeIn{0%{opacity:0;mix-blend-mode:plus-lighter}to{opacity:1;mix-blend-mode:plus-lighter}}@keyframes astroFadeOut{0%{opacity:1;mix-blend-mode:plus-lighter}to{opacity:0;mix-blend-mode:plus-lighter}}@keyframes astroSlideFromRight{0%{transform:translate(100%)}}@keyframes astroSlideFromLeft{0%{transform:translate(-100%)}}@keyframes astroSlideToRight{to{transform:translate(100%)}}@keyframes astroSlideToLeft{to{transform:translate(-100%)}}@media (prefers-reduced-motion){::view-transition-group(*),::view-transition-old(*),::view-transition-new(*){animation:none!important}[data-astro-transition-scope]{animation:none!important}}
</style><style>[data-astro-transition-scope="astro-yzcbpkil-1"] { view-transition-name: title-attention-is-all-you-need; }@layer astro { ::view-transition-old(title-attention-is-all-you-need) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }::view-transition-new(title-attention-is-all-you-need) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }[data-astro-transition=back]::view-transition-old(title-attention-is-all-you-need) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition=back]::view-transition-new(title-attention-is-all-you-need) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; } }[data-astro-transition-fallback="old"] [data-astro-transition-scope="astro-yzcbpkil-1"],
			[data-astro-transition-fallback="old"][data-astro-transition-scope="astro-yzcbpkil-1"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition-fallback="new"] [data-astro-transition-scope="astro-yzcbpkil-1"],
			[data-astro-transition-fallback="new"][data-astro-transition-scope="astro-yzcbpkil-1"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }[data-astro-transition=back][data-astro-transition-fallback="old"] [data-astro-transition-scope="astro-yzcbpkil-1"],
			[data-astro-transition=back][data-astro-transition-fallback="old"][data-astro-transition-scope="astro-yzcbpkil-1"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition=back][data-astro-transition-fallback="new"] [data-astro-transition-scope="astro-yzcbpkil-1"],
			[data-astro-transition=back][data-astro-transition-fallback="new"][data-astro-transition-scope="astro-yzcbpkil-1"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }</style></head> <body>  <div class="container" data-astro-cid-kk57j3mo> <a href="../../" data-astro-cid-kk57j3mo> <h3 data-astro-cid-kk57j3mo>Machine Learning Papers</h3> </a> <h1 data-astro-cid-kk57j3mo data-astro-transition-scope="astro-yzcbpkil-1">Atention is all you need</h1> <code data-astro-cid-kk57j3mo>Ashish Vaswani / Noam Shazeer / Niki Parmar / Jakob Uszkoreit / Llion Jones / Aidan N. Gomez / Łukasz Kaiser / Illia Polosukhin</code> <a href="https://arxiv.org/pdf/1706.03762v6" rel="noopener noreferrer" data-astro-cid-kk57j3mo> <button class="btn" data-astro-cid-kk57j3mo> <span data-astro-cid-kk57j3mo>Leer paper completo</span> </button> </a> <code class="resume" data-astro-cid-kk57j3mo>Resumen</code> <div class="content" data-astro-cid-kk57j3mo> <h1 id="atention-is-all-you-need">Atention is all you need</h1>
<p>Ok, prepárate porque este paper (“Attention Is All You Need” de Google) fue como un terremoto en el mundo de la inteligencia artificial, especialmente en cómo las máquinas entienden y generan lenguaje (eso que llamamos Procesamiento del Lenguaje Natural o NLP). ¡Vamos a desglosarlo de forma sencilla!</p>
<h2 id="introducción-el-problemilla-que-había-antes">Introducción: El Problemilla que Había Antes</h2>
<p>Imagina que quieres que una máquina traduzca un texto largo. Antes de este paper, los modelos más populares eran las <strong>Redes Neuronales Recurrentes (RNNs)</strong>, como las LSTMs o GRUs.</p>
<ul>
<li><strong>¿Cómo funcionaban?</strong> Como si leyeran palabra por palabra, en orden. Tenían una especie de “memoria” a corto plazo para recordar lo que acababan de leer.</li>
<li><strong>El Problema:</strong>
<ol>
<li><strong>Lentitud:</strong> Tenían que procesar las palabras una tras otra. ¡No podías procesar toda la frase de golpe! Esto hacía el entrenamiento súper lento, especialmente con textos largos. Piensa en leer una novela palabra por palabra sin poder saltar ni procesar párrafos enteros a la vez.</li>
<li><strong>Olvidadizas:</strong> Aunque tenían memoria, les costaba un montón recordar la conexión entre palabras que estaban muy separadas en una frase larga (las famosas “dependencias a largo plazo”). Si una palabra al final de un párrafo dependía de una al principio, la RNN a menudo ya se había “olvidado”.</li>
<li><strong>CNNs (Convolucionales):</strong> También se usaron, eran más rápidas para algunas cosas, pero también tenían limitaciones para captar relaciones a distancia.</li>
</ol>
</li>
</ul>
<p><strong>La Propuesta del Paper:</strong> ¿Y si… tiramos todo eso a la basura (la recurrencia y las convoluciones para procesar la secuencia) y usamos <em>solo</em> un mecanismo llamado <strong>Atención</strong>? ¡Spoiler: funcionó de maravilla!</p>
<h2 id="el-modelo-transformer-una-nueva-arquitectura">El Modelo Transformer: Una Nueva Arquitectura</h2>
<p>El corazón del paper es una nueva arquitectura llamada <strong>Transformer</strong>. Olvídate del “paso a paso” de las RNNs.</p>
<ul>
<li>
<p><strong>Estructura General:</strong> Mantiene la idea clásica de <strong>Encoder-Decoder</strong> (Codificador-Decodificador), muy usada en traducción:</p>
<ul>
<li><strong>Encoder (Codificador):</strong> Lee la frase de entrada (ej: en inglés) y la convierte en una representación numérica llena de significado (un conjunto de vectores). Piensa en él como el equipo que <em>entiende</em> la frase original.</li>
<li><strong>Decoder (Decodificador):</strong> Toma esa representación numérica y genera la frase de salida (ej: en español), palabra por palabra. Es el equipo que <em>escribe</em> la traducción.</li>
</ul>
</li>
<li>
<p><strong>¿Qué hay Dentro? (Aquí viene lo nuevo):</strong></p>
<ul>
<li>Tanto el Encoder como el Decoder están formados por varias <strong>capas idénticas</strong> apiladas (en el paper usan 6).</li>
<li>Cada capa tiene <strong>dos sub-capas principales</strong>:
<ol>
<li><strong>Mecanismo de Multi-Head Self-Attention:</strong> ¡La estrella del show! Hablaremos de esto en detalle. Básicamente, permite a cada palabra “mirar” a todas las otras palabras de la frase (incluso a sí misma) para entender mejor su contexto.</li>
<li><strong>Feed-Forward Network (Red Neuronal Simple):</strong> Una red neuronal normalita que procesa la salida de la capa de atención de forma independiente para cada posición.</li>
</ol>
</li>
<li><strong>Trucos extra:</strong> Usan <strong>conexiones residuales</strong> (“atajos” que ayudan a que la información fluya mejor y el entrenamiento sea más fácil) y <strong>normalización de capas</strong> (ayuda a estabilizar los números durante el entrenamiento).</li>
</ul>
</li>
</ul>
<p><strong>La Gran Diferencia:</strong> ¡No hay recurrencia! Todas las palabras se procesan <em>a la vez</em>. La forma en que el modelo entiende las relaciones entre palabras es <em>únicamente</em> a través del mecanismo de atención.</p>
<h2 id="mecanismo-de-atención-la-clave-de-todo">Mecanismo de Atención: ¡La Clave de Todo!</h2>
<p>Aquí está la magia. ¿Cómo funciona eso de “prestar atención”?</p>
<h3 id="scaled-dot-product-attention-atención-por-producto-escalar-escalado">Scaled Dot-Product Attention (Atención por Producto Escalar Escalado)</h3>
<p>Es el bloque de construcción básico. Imagina que tienes una palabra (la llamaremos <strong>Query</strong> o Consulta) y quieres saber cuánto debería importarle cada <em>otra</em> palabra de la frase.</p>
<ol>
<li>
<p><strong>Queries, Keys, Values (Consultas, Claves, Valores):</strong> Cada palabra en la secuencia tiene asociados tres vectores que el modelo aprende:</p>
<ul>
<li><strong>Query (Q):</strong> Representa la palabra actual que está “preguntando” o buscando información. “Oye, ¿quién es relevante para mí?”.</li>
<li><strong>Key (K):</strong> Representa una “etiqueta” o descriptor de una palabra. Responde a la Query diciendo “Yo soy relevante para esto”.</li>
<li><strong>Value (V):</strong> Representa el contenido o significado real de esa palabra. Es la información que se usará si la palabra resulta relevante.</li>
</ul>
</li>
<li>
<p><strong>El Proceso (¡Simplificado!):</strong></p>
<ul>
<li><strong>Paso 1: Comparar:</strong> La Query de una palabra se compara con la Key de <em>todas</em> las palabras (incluida ella misma). ¿Cómo? Calculando el <strong>producto escalar</strong> (dot product) entre el vector Q y cada vector K. Un producto escalar alto significa “¡Oye, esta Key (palabra) parece relevante para mi Query (palabra actual)!”.</li>
<li><strong>Paso 2: Escalar:</strong> Los resultados se dividen por la raíz cuadrada de la dimensión de los vectores Key (<code>sqrt(dk)</code>). Esto es un truco técnico para que el entrenamiento sea más estable, ¡no te preocupes demasiado por el porqué ahora!</li>
<li><strong>Paso 3: Ponderar (Softmax):</strong> Se aplica una función <strong>Softmax</strong> a estos resultados escalados. Esto convierte las puntuaciones en porcentajes o “pesos” que suman 1. Un peso alto significa “esta palabra es súper importante”, un peso bajo significa “meh, no tanto”.</li>
<li><strong>Paso 4: Combinar:</strong> Se multiplican los pesos obtenidos por los vectores <strong>Value (V)</strong> de cada palabra y se suman todos. El resultado es un nuevo vector para la palabra original, que ahora contiene información ponderada de todas las palabras relevantes de la frase. ¡Es como si la palabra hubiera “absorbido” contexto!</li>
</ul>
</li>
</ol>
<h3 id="self-attention-auto-atención">Self-Attention (Auto-Atención)</h3>
<p>Es simplemente aplicar el mecanismo anterior donde las <strong>Queries, Keys y Values provienen de la <em>misma</em> secuencia de entrada</strong>.</p>
<ul>
<li><strong>¿Para qué sirve?</strong> Permite que cada palabra en la frase de entrada (o salida) preste atención a todas las demás palabras <em>en esa misma frase</em>. Así, puede entender el contexto interno. Por ejemplo, en “El gato persiguió al perro porque <em>estaba</em> cansado”, la auto-atención podría ayudar a “estaba” a determinar si se refiere al gato o al perro mirando las otras palabras.</li>
</ul>
<h3 id="multi-head-attention-atención-multi-cabeza">Multi-Head Attention (Atención Multi-Cabeza)</h3>
<p>En lugar de hacer la atención una sola vez con un conjunto de Q, K, V, ¡lo hacen <strong>varias veces en paralelo</strong> (en el paper usan 8 “cabezas”)!</p>
<ul>
<li><strong>¿Por qué?</strong> Cada “cabeza” aprende a enfocarse en <strong>diferentes tipos de relaciones</strong> o aspectos de la frase. Una cabeza podría fijarse en relaciones sintácticas (sujeto-verbo), otra en relaciones semánticas (sinónimos), otra en conexiones a larga distancia, etc.</li>
<li><strong>¿Cómo funciona?</strong>
<ol>
<li>Los vectores Q, K, V originales se dividen y transforman linealmente para cada cabeza.</li>
<li>Cada cabeza realiza el “Scaled Dot-Product Attention” por separado y en paralelo.</li>
<li>Los resultados de todas las cabezas se concatenan (se pegan) y se vuelven a transformar linealmente para obtener el resultado final.</li>
</ol>
</li>
</ul>
<p><strong>Analogía:</strong> Imagina que estás analizando una escena compleja. En lugar de tener una sola persona mirando, tienes 8 expertos (cabezas). Uno es experto en colores, otro en formas, otro en movimiento… Cada uno se fija en lo suyo. Al final, juntan sus informes (concatenan) para tener una comprensión completa y rica de la escena.</p>
<h2 id="positional-encoding-y-el-orden-de-las-palabras">Positional Encoding: ¿Y el Orden de las Palabras?</h2>
<p>Si procesamos todo a la vez, ¿cómo sabe el modelo si una palabra va antes o después? ¡El orden importa! (“El perro muerde al hombre” vs. “El hombre muerde al perro”).</p>
<p>Como el Transformer no tiene recurrencia ni convoluciones que naturalmente procesen el orden, necesita una forma de <strong>inyectar información sobre la posición</strong> de cada palabra.</p>
<ul>
<li><strong>La Solución: Positional Encodings (Codificaciones Posicionales)</strong>
<ul>
<li>Antes de meter los vectores de las palabras (embeddings) en la primera capa, se les <strong>suma</strong> otro vector: el <em>Positional Encoding</em>.</li>
<li>Este vector <strong>no se aprende</strong>, se calcula usando una fórmula fija con funciones seno y coseno de diferentes frecuencias.</li>
<li>La fórmula está diseñada para que:
<ul>
<li>Cada posición tenga una codificación única.</li>
<li>El modelo pueda aprender fácilmente a atender a <strong>posiciones relativas</strong>. Es decir, puede saber qué tan lejos está una palabra de otra.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>En resumen:</strong> Es como añadirle a cada palabra una “etiqueta GPS” matemática que le dice al modelo dónde está ubicada en la secuencia.</p>
<h2 id="ventajas-del-transformer-por-qué-fue-un-bombazo">Ventajas del Transformer: ¿Por Qué Fue un Bombazo?</h2>
<p>Comparado con las RNNs y CNNs anteriores, el Transformer tenía varias ventajas clave:</p>
<ol>
<li><strong>¡Paralelización Total!</strong> Como cada palabra se procesa en paralelo gracias a la atención, se puede aprovechar mucho mejor el hardware moderno (GPUs/TPUs). Esto significa <strong>entrenamientos muchísimo más rápidos</strong>.</li>
<li><strong>Mejor Captura de Dependencias a Largo Plazo:</strong> La atención conecta directamente cualquier par de palabras en la frase, sin importar lo lejos que estén. La “distancia” que tiene que recorrer la información entre dos palabras es constante (O(1)), mientras que en una RNN era proporcional a la distancia (O(n)). ¡Adiós al problema de olvidar!</li>
<li><strong>Rendimiento Superior (Estado del Arte):</strong> En las tareas de traducción en las que lo probaron (WMT Inglés-Alemán e Inglés-Francés), superó a todos los modelos anteriores, ¡y con menos tiempo de entrenamiento!</li>
<li><strong>Generalización:</strong> Aunque nació para traducción, la arquitectura resultó ser muy potente y se adaptó luego a muchísimas otras tareas de NLP (resumen de texto, respuesta a preguntas, generación de texto como GPT, clasificación como BERT…).</li>
</ol>
<h2 id="resultados-y-conclusiones-funcionó">Resultados y Conclusiones: ¡Funcionó!</h2>
<ul>
<li><strong>Resultados Concretos:</strong> El paper mostró mejoras significativas en las puntuaciones BLEU (una métrica común para evaluar traducción automática) en los datasets estándar WMT 2014. Establecieron un nuevo <em>estado del arte</em>.</li>
<li><strong>Impacto:</strong> El Transformer no solo fue un modelo exitoso, sino que <strong>cambió el paradigma</strong> en NLP. Casi todos los modelos grandes y potentes que vinieron después (BERT, GPT-2, GPT-3, T5, etc.) se basan en la arquitectura Transformer.</li>
<li><strong>Conclusión del Paper:</strong> Demostraron que una arquitectura basada <em>únicamente</em> en atención, sin recurrencia ni convoluciones para manejar la secuencia, no solo era viable sino superior para tareas de traducción, abriendo la puerta a futuras investigaciones y modelos más potentes.</li>
</ul>
<p><strong>En pocas palabras:</strong> El paper “Attention Is All You Need” introdujo el Transformer, un modelo que revolucionó el NLP al reemplazar la recurrencia secuencial con la atención paralela, permitiendo entrenamientos más rápidos y una mejor comprensión de las relaciones entre palabras, incluso a larga distancia. ¡Y todo empezó con esa simple idea: la atención es todo lo que necesitas!</p> </div> </div>  </body></html> 